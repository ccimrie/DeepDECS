---
layout: post
title:  "Building Ensembles"
date:   2020-04-07 11:00:00 +0000
---

<p>As I highlighted in my last blog article the stochastic nature of models makes it difficult to know in advance if the model you have selected is the best possible model that could have been generated from that model structure. Like people models have their own biases and, like people, the opinion of a single model can be validated by consulting other models who also have an opinion.
</p>

<p>
So let’s start by looking at a problem set commonly used in the neural network community, CIFAR-10. This data set is a 10-class classifier (like MNIST) but the images are in colour and the classes represented are:
</p>
<pre>
	<code>
class_names = ("plane", "car", "bird", "cat", "deer", "dog", "frog", horse", "ship", "truck")
	</code>
</pre>
<p>
Helpfully the people at Keras provide an example structure for a model to learn these classes from the data which is available:
<a href="https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py">CIFAR-10 model</a>
</p>
<p>
So using this model let’s start by creating 10 models (without image augmentation) and calculate the macro-precision and macro-recall for each model. Don’t worry too much about these measures at the moment, just remember that for both bigger is better and that there is a trade-off to be had between them. For the 10 models I created the resulting performance is:
</p>
<p>
<center>
<img src="{{site.baseurl}}/images/blog/ensembles/performance_1.png" width = 400px/><br/>
Performance of 10 individual models
</center>
</p>
<p>
	We can see that the precision and recall for all models is close (within a couple of percent) but there is one model (in this case) which wins on both precision and recall. Maybe we should take this model and move on? Maybe we can do better with little effort.
</p>
<p>
	This is where ensembles come in. Rather than taking the output of the model in isolation we instead present all models with an input and each model gets to cast a  for the output they think is correct. Each of these votes is then considered before a final decision is made. In the most simple case we just take the majority vote. The result is a model which easily out performs all models in the ensemble:
</p>
<p>
<center>
<img src="{{site.baseurl}}/images/blog/ensembles/performance_2.png" width = 400px/><br/>
Performance of 10 individual models compared to ensemble
</center>
</p>
<p>
	This ensemble is greater than the sum of it parts.
</p>

